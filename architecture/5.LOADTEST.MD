# Load Testing Report & Recommendations

## 1. Executive Summary

Load testing was performed on the `/query` endpoint using **Locust** to evaluate system performance and stability under concurrency.

- **Baseline Test (10 users)**: Passed, but high latency (Median ~9s).
- **Stress Test (50 users)**: Failed with high error rate (57% failure).

## 2. Methodology

- **Target**: `POST /query` (Agentic RAG pipeline)
- **Scenarios**:
    1.  **Baseline**: 10 concurrent users, spawn rate 2/s, duration 30s.
    2.  **Stress**: 50 concurrent users, spawn rate 5/s, duration 60s.

## 3. Results

| Metric | Baseline (10 Users) | Stress (50 Users) |
| :--- | :--- | :--- |
| **Total Requests** | 20 | 405 |
| **Failures** | 0 (0%) | 233 (57.5%) |
| **RPS** | 0.7 | 6.8 |
| **Median Response Time** | ~8.8s | ~2.1s (Skewed by fast 500 errors) |
| **P95 Response Time** | ~27s | ~21s |

### Failure Analysis
The stress test resulted in a significant number of **500 Internal Server Errors**.
- **Root Cause**: The application uses **SQLite** (`app.db`) for storing query history. SQLite is not designed for high-concurrency write operations, leading to locking contention (`database is locked`) when multiple threads attempt to write to `query_history` simultaneously.

## 4. Bottlenecks

1.  **Database Locking (Critical)**: SQLite cannot handle concurrent writes from 50+ users.
2.  **High Latency**: The median response time of ~9s indicates slowness in the agent execution chain (Planner -> Tools -> LLM).
3.  **Vector DB**: Using Milvus Lite (embedded) may also become a bottleneck similar to SQLite under higher loads.

## 5. Recommendations

### Immediate Actions (Scaling)
1.  **Migrate to PostgreSQL**: Replace SQLite with PostgreSQL to handle concurrent reads/writes for `query_history` and `feedback` tables.
2.  **Use Milvus Standalone**: Move from Milvus Lite (embedded) to a standalone Dockerized Milvus instance for better performance and concurrency.

### Optimization
1.  **Async/Parallel Execution**: Review the LangGraph agent execution. If independent search tools (Tavily, Vector Search) are used, run them in parallel to reduce total latency.
2.  **Caching**: Implement a semantic cache (e.g., Redis) to return cached answers for similar queries, bypassing the expensive agent/LLM chain.
3.  **Rate Limiting**: Implement rate limiting at the API level (FastAPI) to prevent system overload.
