# Testing Strategy

## Overview

The testing strategy for the QuanXAI project follows a **Test Pyramid** approach, ensuring robustness at different levels of granularity. We use **Table-Driven Tests** to cover multiple scenarios efficiently and **DeepEval** for evaluating the quality of AI Agent responses.

---

## Test Pyramid

```
      / \
     /   \
    /     \
   /  E2E  \     Level 3: Evaluation (LLM-judged)
  /_________\    (eval/test_agent_evaluation.py)
 /           \
/ Integration \  Level 2: Workflow & Orchestration
/_____________\  (tests/integration/)
/               \
/     Unit      \ Level 1: Components & Tools
/_________________\ (tests/unit/)
```

### Level 1: Unit Tests (Fast & Deterministic)
- **Scope**: Individual tools, helper functions, and isolated components.
- **Approach**: Mock external dependencies (Milvus, Web Search, LLM) to test logic in isolation.
- **Location**: `tests/unit/`

### Level 2: Integration Tests (Graph Flow)
- **Scope**: Agent workflow, state management, and routing logic.
- **Approach**: Mock sub-agents (e.g., Worker Nodes) but test the `Planner` and `Graph` orchestration to ensure correct decision making.
- **Location**: `tests/integration/`

### Level 3: Evaluation (Quality & Performance)
- **Scope**: End-to-End agent responses against synthetic or real datasets.
- **Approach**: Use LLM-as-a-Judge (DeepEval) to measure semantic metrics like Answer Relevancy, Faithfulness, and Hallucination.
- **Location**: `eval/`

---

## Testing Methodology

### Table-Driven Tests
We prefer **Table-Driven Tests** (using `pytest.mark.parametrize`) to separate test data from test logic. This improves readability and makes it easy to add new test cases.

**Example**:
```python
@pytest.mark.parametrize("test_case", [
    {
        "name": "basic_search",
        "query": "headphones",
        "expected_count": 1
    },
    {
        "name": "empty_result",
        "query": "nonexistent",
        "expected_count": 0
    }
])
async def test_search(test_case, tool):
    result = await tool.run(test_case["query"])
    assert len(result) == test_case["expected_count"]
```

### Pytest Fixtures
We use `conftest.py` to define shared fixtures, such as:
- `mock_milvus`: Mocks the Vector DB client.
- `mock_agent_deps`: Mocks internal agents for integration tests.
- `sample_product`: Provides consistent test data.

---

## Directory Structure

```
tests/
├── conftest.py               # Shared fixtures (mocks, sample data)
├── unit/
│   └── test_tools.py         # Unit tests for Tool logic
└── integration/
    └── test_agent_flow.py    # Integration tests for Graph routing

eval/
├── generate_test_data.py     # Generates synthetic test cases
├── test_agent_evaluation.py  # DeepEval metrics tests
└── datasets/
    └── golden_dataset.json   # Test data source
```

---

## Running Tests

### 1. Run Unit & Integration Tests
Fast tests to verify logic and flow.
```bash
uv run pytest tests/ -v
```

### 2. Run Evaluation Tests (LLM)
Slower tests to measure response quality (requires OpenAI API key).
```bash
uv run pytest eval/test_agent_evaluation.py -v
```

### 3. Generate New Test Data
Create new synthetic cases for evaluation.
```bash
uv run python -m eval.generate_test_data
```

---

## Best Practices

1. **Mock External Calls**: Always mock DB and API calls in `tests/` to keep them fast and free.
2. **Fail Fast**: Write unit tests to catch logic errors before they reach integration tests.
3. **Parametrize**: Use table-driven tests to cover edge cases without duplicating code.
4. **Separate Evaluation**: Keep semantic/quality evaluation in `eval/` as it involves cost and latency.
