# Evaluation Architecture

## Overview

The Evaluation System uses **DeepEval** framework to assess the quality of AI Agent responses. It provides automated testing with industry-standard LLM evaluation metrics, synthetic test data generation, and comprehensive reporting.

---

## Evaluation Flow

```
┌─────────────────────────────────────────────────────────────────┐
│                     EVALUATION PIPELINE                         │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────┐  │
│  │   Generate   │───▶│    Agent     │───▶│     DeepEval     │  │
│  │  Test Cases  │    │   Response   │    │     Metrics      │  │
│  └──────────────┘    └──────────────┘    └──────────────────┘  │
│         │                   │                     │             │
│         ▼                   ▼                     ▼             │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────────┐  │
│  │   Synthetic  │    │    Query +   │    │  Score + Reason  │  │
│  │    Inputs    │    │    Answer    │    │    per Metric    │  │
│  └──────────────┘    └──────────────┘    └──────────────────┘  │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## Core Components

### 1. Synthetic Test Generator (`generate_test_data.py`)

**Role**: Creates realistic test cases using DeepEval's Synthesizer with AI-powered generation.

**Test Categories**:

| Category | Count | Description |
|----------|-------|-------------|
| Product QA | 20 | Product catalog queries (RAG) |
| Market Analysis | 15 | Web search queries |
| Multi-tool | 15 | Complex queries requiring both tools |

**Configuration**:
```python
styling_config = StylingConfig(
    input_format="Questions about product catalog",
    expected_output_format="Natural language answer with product details",
    task="Answer product questions using RAG",
    scenario="Product managers asking about inventory"
)
```

---

### 2. Evaluation Metrics (`test_agent_evaluation.py`)

**Metrics Used**:

| Metric | Purpose | Threshold | Input Required |
|--------|---------|-----------|----------------|
| **AnswerRelevancy** | Does answer address query? | 0.7 | input, actual_output |
| **Faithfulness** | Grounded in retrieved context? | 0.7 | + retrieval_context |
| **GEval (Correctness)** | Factually correct? | 0.7 | Custom criteria |
| **Hallucination** | Fabricated information? | 0.5 | + context |

**Metric Flow**:
```
User Query
    │
    ▼
┌─────────────────────┐
│   Agent Response    │
│   - answer          │
│   - products[]      │
│   - sources[]       │
└─────────────────────┘
    │
    ▼
┌─────────────────────┐
│   LLMTestCase       │
│   - input           │
│   - actual_output   │
│   - retrieval_ctx   │
└─────────────────────┘
    │
    ▼
┌─────────────────────┐
│   GPT-4o Judge      │
│   Score + Reason    │
└─────────────────────┘
```

---

### 3. Test Runner

**Prerequisites**:

```bash
# 1. Ingest data first (creates Milvus collection)
uv run python -m src.cli.ingest --sync   # smart sync

# 2. Start Docker (uses the ingested data)
docker-compose up --build -d
```

> [!IMPORTANT]
> You must ingest data before running Docker. The evaluation tests require the `products` collection in Milvus.

**Running Tests**:

```bash
# Generate test dataset (15 cases)
uv run python -m eval.generate_test_data

# Run evaluation with pytest
uv run pytest eval/test_agent_evaluation.py -v

# Run with DeepEval CLI
uv run deepeval test run eval/test_agent_evaluation.py
```

---

## Metrics Details

### Answer Relevancy

Evaluates if the answer addresses the user's query.

```python
AnswerRelevancyMetric(
    threshold=0.7,
    model="gpt-4o",
    include_reason=True
)
```

**Example**:
- Query: "What wireless headphones do we have?"
- Answer: "We have Sony WH-1000XM5 and Bose QC45..."
- Score: 0.92 ✅

---

### Faithfulness

Checks if the answer is grounded in retrieved context (products/sources).

```python
FaithfulnessMetric(
    threshold=0.7,
    model="gpt-4o",
    include_reason=True
)
```

**Example**:
- Context: `[{"name": "Sony WH-1000XM5", "price": 12990}]`
- Answer: "Sony headphones cost 12,990 THB"
- Score: 1.0 ✅ (grounded)

---

### GEval (Correctness)

Custom metric using Chain-of-Thought evaluation.

```python
GEval(
    name="Correctness",
    criteria="Is the answer factually correct?",
    threshold=0.7,
    model="gpt-4o"
)
```

---

### Hallucination

Detects fabricated information not present in context.

```python
HallucinationMetric(
    threshold=0.5,  # Lower is better
    model="gpt-4o",
    include_reason=True
)
```

**Example**:
- Context: "Sony headphones cost 12,990 THB"
- Answer: "Sony headphones cost 9,990 THB" ❌
- Score: 0.8 (high hallucination)

---

## Test Case Structure

```json
{
  "metadata": {
    "total_cases": 50,
    "categories": {
      "product_qa": 20,
      "market_analysis": 15,
      "multi_tool": 15
    }
  },
  "test_cases": [
    {
      "input": "What wireless headphones do we have in stock?",
      "expected_output": "We have several wireless headphones...",
      "category": "product_qa"
    }
  ]
}
```

---

## File Structure

```
eval/
├── __init__.py
├── generate_test_data.py       # Synthetic data generation
├── test_agent_evaluation.py    # Main evaluation tests
├── datasets/
│   └── golden_dataset.json     # Generated test cases
└── README.md                   # Usage documentation
```

---

## Success Criteria

| Metric | Target | Description |
|--------|--------|-------------|
| Answer Relevancy | ≥ 0.7 | 70% of answers address query |
| Faithfulness | ≥ 0.7 | 70% answers grounded in data |
| Correctness | ≥ 0.7 | 70% factually correct |
| Hallucination | ≤ 0.5 | Less than 50% hallucination |

---

## Future Improvements
 
1. **Baseline Tracking**: Track metrics over time to detect regressions
2. **Conversational Metrics**: Add multi-turn conversation evaluation 
